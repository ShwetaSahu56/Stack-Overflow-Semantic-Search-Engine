{"cells":[{"cell_type":"markdown","source":["**Implement Word2vec with skipgrams using negative sampling on selected Questions** "],"metadata":{"id":"3o7naQKcPsrB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ul6MIbR7yH7J"},"outputs":[],"source":["from collections import Counter\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from tqdm.notebook import tqdm\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import TextVectorization, Dense,Input,Activation,Embedding, Dot, Flatten, Dropout\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.initializers\n","from sklearn.metrics import f1_score\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.losses import BinaryCrossentropy\n","import h5py\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","import io"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XodLK8SUPpP2"},"outputs":[],"source":["#get the preprocessed selected questions data\n","pre_processed_data = pd.read_pickle(\"/content/drive/MyDrive/StackOverflow_CaseStudy/Preprocessed_selected_data.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":620,"status":"ok","timestamp":1673426117476,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"acNJpMV9v-br","outputId":"3838a081-76e7-4bb7-c4e8-292b10d1d9a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(296099, 8)"]},"metadata":{},"execution_count":13}],"source":["pre_processed_data.shape"]},{"cell_type":"markdown","source":["**Check the number of words for each question to decide the maximum length**"],"metadata":{"id":"n-JR5C9Tgy63"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MohHiiAReJQt"},"outputs":[],"source":["word_count = [len(str(x).split()) for x in list(pre_processed_data['Ques_Text'].values)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":997,"status":"ok","timestamp":1673357935913,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"3K6c2cJceJGb","outputId":"b158c486-c1c7-439a-a736-1472d7821b31"},"outputs":[{"name":"stdout","output_type":"stream","text":["90 percentile value is 195\n","91 percentile value is 203\n","92 percentile value is 211\n","93 percentile value is 222\n","94 percentile value is 233\n","95 percentile value is 247\n","96 percentile value is 265\n","97 percentile value is 290\n","98 percentile value is 326\n","99 percentile value is 398\n","100 percentile value is  10245\n"]}],"source":["for i in range(90,100):\n","    var = sorted(word_count)\n","    #var = np.sort(var,axis = None)\n","    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\n","print (\"100 percentile value is \",var[-1])"]},{"cell_type":"markdown","source":["We will keep the max length as 200 since it can cover more than 90% of the questions"],"metadata":{"id":"SXd6qqyZhGsx"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12617,"status":"ok","timestamp":1673436322082,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"08pXEwLphoVu","outputId":"4a89e29c-51e3-4ef2-a881-2d7f81af1003"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["not                   738796\n","code                  329440\n","use                   320126\n","using                 274556\n","like                  255820\n","                       ...  \n","dftwiki                    1\n","csc231                     1\n","imagicon                   1\n","compile-time-error         1\n","datecal                    1\n","Length: 486804, dtype: int64"]},"metadata":{},"execution_count":3}],"source":["#Word frequency across the corpus\n","word_freq_corpus = pd.Series(' '.join(pre_processed_data.Ques_Text).split()).value_counts()\n","word_freq_corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":455,"status":"ok","timestamp":1673436324842,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"cewSvpGPiVzc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f129c90-d8c7-4b41-9460-96e03b8d2522"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["40278"]},"metadata":{},"execution_count":4}],"source":["#We will consider the words occuring more than 10 times in the corpus\n","vocab_size = len(word_freq_corpus[word_freq_corpus.values>10])\n","vocab_size"]},{"cell_type":"code","source":["vocab_size=40278"],"metadata":{"id":"_0F0zPOYHb7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Prepare the Question text dataset**"],"metadata":{"id":"k4--hHlchw3z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncX-hEi-eI3O"},"outputs":[],"source":["question_text = tf.data.Dataset.from_tensor_slices(list(pre_processed_data['Ques_Text'].values))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCiJM9jIizsB"},"outputs":[],"source":["max_ques_text_length = 200\n","vectorize_layer = TextVectorization(standardize=None, max_tokens=vocab_size, output_mode='int', output_sequence_length=max_ques_text_length)\n","vectorize_layer.adapt(question_text.batch(1024))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1drscxUn_J4"},"outputs":[],"source":["inverse_vocab = vectorize_layer.get_vocabulary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiDBr5N1h9a3"},"outputs":[],"source":["# Vectorize all the questions in question_text.\n","question_text = question_text.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26879,"status":"ok","timestamp":1673409999206,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"zIlKweJNh9JE","outputId":"9b4ba798-a8a6-4c5d-f3fd-cec8169e8ae9"},"outputs":[{"output_type":"stream","name":"stdout","text":["296099\n"]}],"source":["question_sequences = list(question_text.as_numpy_iterator())\n","print(len(question_sequences))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598,"status":"ok","timestamp":1673358176478,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"zatH048cpXsq","outputId":"ceead9b5-d6ed-4365-a6f1-9d89d8b345f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[  245  2543   575    59   789   396   583   431    53   278    95  4298\n","  1467 12542  1633   583   644   236    19  3063   149   445  1252   242\n","  1633    12  1089   251    19   111   583    29   227  5160     1     7\n","     6    12  1633   197     3    95  1252   539    31     4    30    54\n","  1633    30    95   382    47   889    93  2834   709  1858  1240   106\n","     7  1758   709   503  1633   273  3462  1889    94  1078    12   197\n","    15  1427   213   479    53   776  4137    94   120   149    59   479\n"," 15241     1    53   149    78     3    84   133   635  3991  1807   149\n","  9022    47    70   276     7   484   479  1052   883  2543 16726  7076\n","  5890  7076 16726   333  1600  1225  4221  7076   102  1946  7076   149\n","   330   536    88   347   403  1054  5890  2543   479  3721    63  3220\n","  5890   482    59   333  3059   154    36   307  3059    56     1  4224\n","   635   102     8   986   854   482    59    70  2786  3744    42  3123\n","  3868   565  4221  3123    47   154  6864    84  1569  3123 10238   127\n","   110     4 16726   550     7   406   362   338 10007  1889    26   141\n","   492   709    12     4   989   160   778  1889   500   133     8   539\n","  3140    26     4  9017    26     4   989   160] => ['adding', 'scripting', 'functionality', 'net', 'applications', 'little', 'game', 'written', 'c', 'uses', 'database', 'back-end', 'na', 'trading', 'card', 'game', 'wanted', 'implement', 'function', 'cards', 'script', 'mean', 'essentially', 'interface', 'card', 'class', 'implements', 'contains', 'function', 'called', 'game', 'make', 'thing', 'maintainable', '[UNK]', 'would', 'like', 'class', 'card', 'source', 'code', 'database', 'essentially', 'compile', 'first', 'use', 'add', 'change', 'card', 'add', 'database', 'tell', 'application', 'refresh', 'without', 'needing', 'assembly', 'deployment', 'especially', 'since', 'would', 'talking', 'assembly', 'per', 'card', 'means', 'hundreds', 'assemblies', 'possible', 'register', 'class', 'source', 'file', 'instantiate', 'etc', 'language', 'c', 'extra', 'bonus', 'possible', 'write', 'script', 'net', 'language', 'oleg', '[UNK]', 'c', 'script', 'solution', 'code', 'project', 'really', 'great', 'introduction', 'providing', 'script', 'abilities', 'application', 'different', 'approach', 'would', 'consider', 'language', 'specifically', 'built', 'scripting', 'ironruby', 'ironpython', 'lua', 'ironpython', 'ironruby', 'available', 'today', 'guide', 'embedding', 'ironpython', 'read', 'embed', 'ironpython', 'script', 'support', 'existing', 'app', '10', 'easy', 'steps', 'lua', 'scripting', 'language', 'commonly', 'used', 'games', 'lua', 'compiler', 'net', 'available', 'codeplex', '--', 'http', 'www', 'codeplex', 'com', '[UNK]', 'codebase', 'great', 'read', 'want', 'learn', 'building', 'compiler', 'net', 'different', 'angle', 'altogether', 'try', 'powershell', 'numerous', 'examples', 'embedding', 'powershell', 'application', '--', 'thorough', 'project', 'topic', 'powershell', 'tunnel', 'might', 'able', 'use', 'ironruby', 'otherwise', 'would', 'suggest', 'directory', 'place', 'precompiled', 'assemblies', 'could', 'reference', 'db', 'assembly', 'class', 'use', 'reflection', 'load', 'proper', 'assemblies', 'runtime', 'really', 'want', 'compile', 'run-time', 'could', 'use', 'codedom', 'could', 'use', 'reflection', 'load']\n","[  455   115   155    97     7     6   155    87    47  3820   255     2\n","    89  1079   575   458   473  3713  1661    83   514 26335     2    10\n","   395     8    21     5   667    15  1671  2280   709   155   345   667\n","   231  1292   173   106    47  1077   667    15    75   362     7     6\n","   110   210   155    97    80  4665    39  2061    22   384   155     5\n","   142   382  2397   552   995   837  1179  5682    62     7    21   408\n","   255  5151    97    10  3820   883   679     2     5   342  5151   255\n","   397  2338   235  1697   653    21   162    16   505    35   709  9643\n","   353   286   455  3787   397     4  9699   376  9699   376  1464   455\n","  1610   255    97  9058   397   155    28  1530 12353   163  3254  3423\n","    10   627   255   397  1530 12353     5  7159   775   530    54     5\n","  4376   430   709   155   505   255    41   713  7159   341  2042   155\n","    97   488   403  2263    59   499    53   255   178   900    97     2\n","  4171    13   690    23  4309  1290    63  2105  2781   995   255   127\n","   769    59  2105  2781   255  5151   593    59  2798   709   155   278\n","    44   427 20468  1023   255   174    97  1048   106  4875  1916  3890\n","  1023  5151   174    97   711   106  5736   340] => ['automatically', 'update', 'version', 'number', 'would', 'like', 'version', 'property', 'application', 'incremented', 'build', 'not', 'sure', 'enable', 'functionality', 'visual', 'studio', '2005', '2008', 'tried', 'specify', 'assemblyversion', 'not', 'get', 'exactly', 'want', 'also', 'using', 'settings', 'file', 'earlier', 'attempts', 'assembly', 'version', 'changed', 'settings', 'got', 'reset', 'default', 'since', 'application', 'looked', 'settings', 'file', 'another', 'directory', 'would', 'like', 'able', 'display', 'version', 'number', 'form', '38', 'user', 'finds', 'problem', 'log', 'version', 'using', 'well', 'tell', 'upgrade', 'old', 'release', 'short', 'explanation', 'versioning', 'works', 'would', 'also', 'appreciated', 'build', 'revision', 'number', 'get', 'incremented', 'built', 'stuff', 'not', 'using', 'replace', 'revision', 'build', 'numbers', 'coded', 'date', 'timestamp', 'usually', 'also', 'good', 'way', 'info', 'see', 'assembly', 'linker', 'documentation', 'tag', 'automatically', 'incrementing', 'numbers', 'use', 'assemblyinfo', 'task', 'assemblyinfo', 'task', 'configured', 'automatically', 'increment', 'build', 'number', 'gotchas', 'numbers', 'version', 'string', 'limited', '65535', 'windows', 'limitation', 'unlikely', 'get', 'fixed', 'build', 'numbers', 'limited', '65535', 'using', 'subversion', 'requires', 'small', 'change', 'using', 'msbuild', 'generate', 'assembly', 'version', 'info', 'build', 'time', 'including', 'subversion', 'fix', 'retrieving', 'version', 'number', 'quite', 'easy', 'clarify', 'net', 'least', 'c', 'build', 'actually', 'third', 'number', 'not', 'fourth', 'one', 'people', 'example', 'delphi', 'developers', 'used', 'major', 'minor', 'release', 'build', 'might', 'expect', 'net', 'major', 'minor', 'build', 'revision', 'vs', 'net', 'defaults', 'assembly', 'version', 'uses', 'following', 'logic', 'auto-incrementing', 'sets', 'build', 'part', 'number', 'days', 'since', 'january', '1st', '2000', 'sets', 'revision', 'part', 'number', 'seconds', 'since', 'midnight', 'local']\n","[  678    95   125 11346    53  1616    16   678   170    95    33   908\n","    53     1   154  3092    40   431   266 21499   154   124   300     2\n","  1312   550   902  3580   266   106     2     3  4107  1411   162    16\n","   712     5    95  1126   429   244   302  2833   101    40     6   909\n","    16  6301   913    13     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0] => ['connect', 'database', 'loop', 'recordset', 'c', 'simplest', 'way', 'connect', 'query', 'database', 'set', 'records', 'c', '[UNK]', '--', 'excellent', 'something', 'written', 'memory', 'ntested', '--', 'found', 'connection', 'not', 'opened', 'otherwise', 'nice', 'roughly', 'memory', 'since', 'not', 'code', 'laptop', 'definitely', 'good', 'way', 'happen', 'using', 'database', 'supports', 'linq', 'sql', 'lot', 'fun', 'look', 'something', 'like', 'alternative', 'way', 'datareader', 'faster', 'one', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n","[   10    20   883  1792  3600     9  1647 12896  1449  3600   872     7\n","     2   333  1381  2746   147  2975   464    23   157    27  1212     9\n","    16    50   270    10    20  2263     9    20    27  1276     2  2142\n","     9    45  3600    20   767   134     2  3600   128   183 34633  1505\n","   162   338   164   214   876  3907  3600   154  4848  3600    50   767\n","   134 28735   395     8  6606  9689  3600  3012  3713  3600  3012   138\n","   793  3012  3149    35   994   212  2185  2982    11  1796   173   559\n","  1376  3600    61   413   304    50   421    18   285    59  3600  1119\n","  5488     1    12    12   951    12  1172   440     1    60   676  5147\n","    12    59   313   650     1     1   173     1    63   304   160  3600\n","   193   619    10     1    25   304  3600   421    18   699     2   113\n","     3   156   158   395     8    16  2973   103  3600     3   130    50\n","  2142  1339  3012    44   275  1521  1084   167  1041  3600    47  1406\n","  1796    11    43   518    11   195   687    30   612   708   427   742\n","   312  3600     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0] => ['get', 'value', 'built', 'encoded', 'viewstate', 'need', 'grab', 'base64-encoded', 'representation', 'viewstate', 'obviously', 'would', 'not', 'available', 'fairly', 'late', 'request', 'lifecycle', 'ok', 'example', 'output', 'page', 'includes', 'need', 'way', 'server', 'side', 'get', 'value', 'clarify', 'need', 'value', 'page', 'rendered', 'not', 'postback', 'need', 'know', 'viewstate', 'value', 'sent', 'client', 'not', 'viewstate', 'getting', 'back', 'rex', 'suspect', 'good', 'place', 'start', 'looking', 'solutions', 'compress', 'viewstate', '--', 'grabbing', 'viewstate', 'server', 'sent', 'client', 'gzipping', 'exactly', 'want', 'scott', 'hanselman', 'viewstate', 'compression', '2005', 'viewstate', 'compression', 'system', 'io', 'compression', '2007', 'see', 'blog', 'post', 'author', 'describes', 'method', 'overriding', 'default', 'behavior', 'generating', 'viewstate', 'instead', 'shows', 'save', 'server', 'session', 'object', 'asp', 'net', 'viewstate', 'saved', 'descendant', '[UNK]', 'class', 'class', 'abstract', 'class', 'saving', 'loading', '[UNK]', 'two', 'implemented', 'descendants', 'class', 'net', 'framework', 'named', '[UNK]', '[UNK]', 'default', '[UNK]', 'used', 'save', 'load', 'viewstate', 'information', 'easily', 'get', '[UNK]', 'work', 'save', 'viewstate', 'session', 'object', 'although', 'not', 'test', 'code', 'seems', 'show', 'exactly', 'want', 'way', 'gain', 'access', 'viewstate', 'code', 'still', 'server', 'postback', 'enabled', 'compression', 'following', 'similar', 'articles', 'posted', 'key', 'accessing', 'viewstate', 'application', 'sends', 'overriding', 'method', 'call', 'base', 'method', 'within', 'override', 'add', 'whatever', 'additional', 'logic', 'require', 'handle', 'viewstate', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n","[  495    15  2993    75   234    53   214    16   495    15  2993    75\n","   234     5    53  1505    11   201   110    72   234  2286    15   766\n","  2427  1542   699     2    89    53   434   234   110   566    15   495\n","     5  5588  1617     2 14539   227   873  2752    40     6 30252    26\n","     4   119    19  1465    15  4781   983   269  6648   648   133     9\n","   495    15     4    75   234     7   645     1   426    22  2086   876\n","  2528    11   491   561     8    53   649     2    45   234    15  2993\n","     9  3905   234   312    55   170   312   846  5729  2993    15    53\n","   741   742   738  7827   674  2856    43   723  1632     9  1362   234\n","  3759    15  2993     9  2960  1238   530   723   486   234    21  1238\n","   866   486 18383   164  1297    59   500  1063   486  2241   312     5\n"," 39874   213  1252    16  6503  2993    15  1238   486  6205   234   521\n","   545   434     5   723   866     3    32   765   237     9   530   818\n","   723     3   499   738  1132   671    36   307  2662    56  2837   414\n","     1   444    36     1    56  1661  3066  2456     1   162  1155     8\n","  2794     2    89     7   133   645  1018  6167   679   574 10554   234\n","  1256    58    67     4    72   312   324    86] => ['delete', 'file', 'locked', 'another', 'process', 'c', 'looking', 'way', 'delete', 'file', 'locked', 'another', 'process', 'using', 'c', 'suspect', 'method', 'must', 'able', 'find', 'process', 'locking', 'file', 'perhaps', 'tracking', 'handles', 'although', 'not', 'sure', 'c', 'close', 'process', 'able', 'complete', 'file', 'delete', 'using', 'killing', 'processes', 'not', 'healthy', 'thing', 'scenario', 'involves', 'something', 'like', 'uninstallation', 'could', 'use', 'api', 'function', 'mark', 'file', 'deletion', 'upon', 'next', 'reboot', 'appears', 'really', 'need', 'delete', 'file', 'use', 'another', 'process', 'would', 'recommend', '[UNK]', 'actual', 'problem', 'considering', 'solutions', 'typical', 'method', 'follows', 'said', 'want', 'c', 'goes', 'not', 'know', 'process', 'file', 'locked', 'need', 'examine', 'process', 'handle', 'list', 'query', 'handle', 'determine', 'identifies', 'locked', 'file', 'c', 'likely', 'require', 'invoke', 'intermediary', 'c++', 'cli', 'call', 'native', 'apis', 'need', 'figured', 'process', 'es', 'file', 'locked', 'need', 'safely', 'inject', 'small', 'native', 'dll', 'process', 'also', 'inject', 'managed', 'dll', 'messier', 'start', 'attach', 'net', 'runtime', 'bootstrap', 'dll', 'closes', 'handle', 'using', 'closehandle', 'etc', 'essentially', 'way', 'unlock', 'locked', 'file', 'inject', 'dll', 'offending', 'process', 'address', 'space', 'close', 'using', 'native', 'managed', 'code', 'no', 'matter', 'going', 'need', 'small', 'amount', 'native', 'code', 'least', 'invoke', 'helpful', 'links', 'http', 'www', 'codeproject', 'com', 'kb', 'threads', '[UNK]', 'aspx', 'http', '[UNK]', 'com', '2008', '07', '02', '[UNK]', 'good', 'luck', 'want', 'programatically', 'not', 'sure', 'would', 'really', 'recommend', 'nif', 'troubleshooting', 'stuff', 'machine', 'sysinternals', 'process', 'explorer', 'help', 'run', 'use', 'find', 'handle', 'command', 'think']\n"]}],"source":["for seq in question_sequences[:5]:\n","  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"]},{"cell_type":"markdown","metadata":{"id":"fX67LGbsLY2Z"},"source":["**Generate training examples (positive and negative skipgrams) from question text**"]},{"cell_type":"markdown","source":["For training Word2Vec with skipgrams we need to generate positive and negative samples of (target word, context word) pair and label (0&1). For postive sample, target and context words occuring together in the corpus is used while for negative samples, context words are randomly generated which has not occured together with the corresponding context word in the corpus. This way multi-class classification problem is converted to binary classification problem."],"metadata":{"id":"CBHrZ4d6oWxt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIxweTviLxja"},"outputs":[],"source":["# Generates skip-gram pairs with negative sampling for a list of sequences\n","# (int-encoded sentences) based on window size, number of negative samples\n","# and vocabulary size.\n","\n","#tensorflow.keras has functions which help in easy generation of positive and negative samples\n","def generate_training_data(sequences, window_size, vocab_size, seed):\n","  # Elements of each training example are appended to this array.\n","  data_array = np.empty((0,3), dtype=int)\n","  # Build the sampling table for `vocab_size` tokens.\n","  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","  \n","  # Iterate over all sequences (sentences) in the dataset.\n","  for sequence in tqdm(sequences):\n","    pairs, label_seq = tf.keras.preprocessing.sequence.skipgrams(sequence,vocabulary_size=vocab_size,\n","          sampling_table=sampling_table, window_size=window_size, negative_samples=2.0, shuffle=True)\n","    \n","    data_seq = np.column_stack((np.array(pairs, dtype=int), np.array(label_seq, dtype=int)))\n","    if(data_seq.shape[1]<3):\n","      data_seq = np.empty((0,3), dtype=int)\n","    data_array = np.vstack((data_array, data_seq))\n","  return data_array"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c4843b561442436e8440defe9769f1e4","f3e93c959be44de08a2f37fb79d1836c","6c3d51137b664b8691d999a95e3df2e3","6a6df92ebf934564ba7a5121cc3f224a","51c46e9b9e27411cbbbb23e6e69e900b","0f0bdd7351074483a59aedc0c1c5af6d","06d4af18340e4efeaca9110613ee2c00","fd012c2bb009453f893fc0c1f59b56f7","a95327ead0e44961ab92b22ed888963a","0baacf17524d4a3f8483409f359aee5a","0fa99e4168bb4c798e5be10f34891fbe"]},"id":"blIYxNjNh8-s","executionInfo":{"status":"ok","timestamp":1673413923411,"user_tz":-330,"elapsed":3924228,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"}},"outputId":"5ac1dabd-d5fa-4ba6-bef6-063202d4860f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/74024 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4843b561442436e8440defe9769f1e4"}},"metadata":{}}],"source":["#performing this operation in batches to avoid running out of memory\n","no_of_batches=4\n","batch_size = int(len(question_sequences)/no_of_batches)\n","model_data = tf.data.Dataset\n","path = \"/content/drive/MyDrive/StackOverflow_CaseStudy/DataFiles/\"\n","\n","for i in range(0,no_of_batches):\n","  #pairs, labels = \n","  data = generate_training_data(sequences=question_sequences[i*batch_size : (i+1)*batch_size],\n","                                                     window_size=2, vocab_size=vocab_size, seed=10)\n","  #save the (target word, context word) pair and labels generated to a hdf5 file\n","  hf = h5py.File(path+str(i)+'_data.hdf5', 'w')\n","  hf.create_dataset('dataset_1', data=data)\n","  hf.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clqb4GPJYJSy"},"outputs":[],"source":["path = \"/content/drive/MyDrive/StackOverflow_CaseStudy/DataFiles/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbBGoNMJXREy"},"outputs":[],"source":["#get the data generated for all the batches\n","hf = h5py.File(path+'0_data.hdf5', 'r')\n","data1 = hf.get('dataset_1')\n","hf = h5py.File(path+'1_data.hdf5', 'r')\n","data2 = hf.get('dataset_1')\n","hf = h5py.File(path+'2_data.hdf5', 'r')\n","data3 = hf.get('dataset_1')\n","hf = h5py.File(path+'3_data.hdf5', 'r')\n","data4 = hf.get('dataset_1')"]},{"cell_type":"code","source":["print(data1.shape)\n","print(data2.shape)\n","print(data3.shape)\n","print(data4.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9NO20TYQTaM","executionInfo":{"status":"ok","timestamp":1673414172251,"user_tz":-330,"elapsed":1082,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"}},"outputId":"373cb416-3ba4-4198-8b40-36c82ae86f60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(21448311, 3)\n","(18481056, 3)\n","(17546679, 3)\n","(17182413, 3)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28454,"status":"ok","timestamp":1673414524591,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"sHPt9xbX_WWD","outputId":"13fb4823-9e70-4970-c0a8-1e6779ce316f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(74658459, 3)"]},"metadata":{},"execution_count":17}],"source":["#combine the data generated for all the batches\n","data = np.vstack((np.array(data1), np.array(data2), np.array(data3), np.array(data4)))\n","data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNBNnOJbKE6Q"},"outputs":[],"source":["targets = data[:, 0]\n","contexts = data[:, 1]\n","labels = data[:, 2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":617,"status":"ok","timestamp":1673415213746,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"fgnfEz0Pad-k","outputId":"75da06de-1379-42e9-bcbc-591eafab341e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(74658459,)\n","(74658459,)\n","(74658459,)\n"]}],"source":["print(targets.shape)\n","print(contexts.shape)\n","print(labels.shape)"]},{"cell_type":"markdown","source":["**Create a simple Deep Learning model to train on above data to generaye word vectors**"],"metadata":{"id":"VK-7xhGuqYb3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0Bolfkj0fYT"},"outputs":[],"source":["embedding_dim = 128\n","BATCH_SIZE = 1024"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1673415438850,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"U33b7xA3AJhG","outputId":"9a0923e3-075e-4419-a9e1-6798b59fb8db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 1, 128)       5155712     ['input_1[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 1, 128)       5155712     ['input_2[0][0]']                \n","                                                                                                  \n"," dot (Dot)                      (None, 128, 128)     0           ['embedding[0][0]',              \n","                                                                  'embedding_1[0][0]']            \n","                                                                                                  \n"," dense (Dense)                  (None, 128, 64)      8256        ['dot[0][0]']                    \n","                                                                                                  \n"," dropout (Dropout)              (None, 128, 64)      0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 128, 32)      2080        ['dropout[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 128, 32)      0           ['dense_1[0][0]']                \n","                                                                                                  \n"," flatten (Flatten)              (None, 4096)         0           ['dropout_1[0][0]']              \n","                                                                                                  \n"," dense_2 (Dense)                (None, 1)            4097        ['flatten[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 10,325,857\n","Trainable params: 10,325,857\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["#input and embedding layer for target words\n","target_word = Input(shape=(1,))\n","#weights of this layer will be the word embeddings \n","target_emb = tf.keras.layers.Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, input_length=1)(target_word)\n","\n","#input and embedding layer for context words\n","context_word = Input(shape=(1,))\n","context_emb = tf.keras.layers.Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, input_length=1)(context_word)\n","\n","#dot product of two vectors gives their similarity (cosine similarity) thats why dot layer is used\n","dot = tf.keras.layers.Dot(axes=1)([target_emb, context_emb])\n","\n","dense1 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(dot)\n","drp1 = tf.keras.layers.Dropout(0.2)(dense1)\n","\n","dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(drp1)\n","drp2 = tf.keras.layers.Dropout(0.2)(dense2)\n","\n","flt = tf.keras.layers.Flatten()(drp2)\n","output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(flt)\n","\n","model =Model(inputs=[target_word, context_word], outputs=output)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0gi6SNu2fVx"},"outputs":[],"source":["#compile the model using Adam optimiser and accuracy as metric\n","model.compile(optimizer='adam', loss = BinaryCrossentropy(), metrics=['accuracy'])\n","\n","#define Tensorboard callback to log the losses and to generate loss and accuracy curve later\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n","\n","#callback to stop the training if validation accuracy is not increased in last 2 epochs\n","earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=2, verbose=1)\n","\n","filepath=\"/content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n","#callback to save model at every epoch if validation accuracy is improved from previous epoch\n","checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='auto')\n","\n","callbacks = [tensorboard_callback, earlystop, checkpoint]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1137979,"status":"ok","timestamp":1673419906516,"user":{"displayName":"Shweta Sahu","userId":"17031201530500255745"},"user_tz":-330},"id":"-TqyxR0S9q-c","outputId":"f0038f83-7c0c-4bee-b8cb-f33ac58271c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","61970/61973 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9128\n","Epoch 1: val_accuracy improved from -inf to 0.91747, saving model to /content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/weights-01-0.9175.hdf5\n","61973/61973 [==============================] - 1172s 19ms/step - loss: 0.2280 - accuracy: 0.9128 - val_loss: 0.2189 - val_accuracy: 0.9175\n","Epoch 2/10\n","61971/61973 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9186\n","Epoch 2: val_accuracy improved from 0.91747 to 0.91974, saving model to /content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/weights-02-0.9197.hdf5\n","61973/61973 [==============================] - 1165s 19ms/step - loss: 0.2138 - accuracy: 0.9186 - val_loss: 0.2137 - val_accuracy: 0.9197\n","Epoch 3/10\n","61971/61973 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9202\n","Epoch 3: val_accuracy improved from 0.91974 to 0.92045, saving model to /content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/weights-03-0.9205.hdf5\n","61973/61973 [==============================] - 1173s 19ms/step - loss: 0.2094 - accuracy: 0.9202 - val_loss: 0.2117 - val_accuracy: 0.9205\n","Epoch 3: early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd893c1d6d0>"]},"metadata":{},"execution_count":37}],"source":["#train the Word2Vec model\n","model.fit([targets, contexts], labels, batch_size=BATCH_SIZE, epochs=10, validation_split=.15, callbacks=callbacks)"]},{"cell_type":"code","source":["#get the model with best performance\n","saved_model = tf.keras.models.load_model('/content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/weights-03-0.9205.hdf5')\n","#get the weights of embedding layer, these are are word vectors for our vocabulary\n","weights = saved_model.get_layer('embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()"],"metadata":{"id":"pQq6BBf1mtvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#write vocab-vector dictionary to a file\n","out_v = io.open('/content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/word_vectors.txt', 'w', encoding='utf-8')\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue  # skip 0, it's padding.\n","  vec = weights[index]\n","  out_v.write(word+' '+' '.join([str(x) for x in vec]) + \"\\n\")\n","out_v.close()"],"metadata":{"id":"h83k_ZPDlOLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#below files can be used to analyse created word vectors in Tensorflow's Embedding projector\n","out_v = io.open('/content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/vectors.tsv', 'w', encoding='utf-8')\n","out_m = io.open('/content/drive/MyDrive/StackOverflow_CaseStudy/Saved_Model/metadata.tsv', 'w', encoding='utf-8')\n","\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue  # skip 0, it's padding.\n","  vec = weights[index]\n","  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","  out_m.write(word + \"\\n\")\n","out_v.close()\n","out_m.close()"],"metadata":{"id":"asNooB3Rpfnk"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"11mFtNER3CbnI5Uo5Ln2SRBkUx-9ugbpp","authorship_tag":"ABX9TyMhmVupzemMo7lbPjG84LIv"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c4843b561442436e8440defe9769f1e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3e93c959be44de08a2f37fb79d1836c","IPY_MODEL_6c3d51137b664b8691d999a95e3df2e3","IPY_MODEL_6a6df92ebf934564ba7a5121cc3f224a"],"layout":"IPY_MODEL_51c46e9b9e27411cbbbb23e6e69e900b"}},"f3e93c959be44de08a2f37fb79d1836c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f0bdd7351074483a59aedc0c1c5af6d","placeholder":"​","style":"IPY_MODEL_06d4af18340e4efeaca9110613ee2c00","value":"100%"}},"6c3d51137b664b8691d999a95e3df2e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd012c2bb009453f893fc0c1f59b56f7","max":74024,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a95327ead0e44961ab92b22ed888963a","value":74024}},"6a6df92ebf934564ba7a5121cc3f224a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0baacf17524d4a3f8483409f359aee5a","placeholder":"​","style":"IPY_MODEL_0fa99e4168bb4c798e5be10f34891fbe","value":" 74024/74024 [1:05:15&lt;00:00, 10.49it/s]"}},"51c46e9b9e27411cbbbb23e6e69e900b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f0bdd7351074483a59aedc0c1c5af6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06d4af18340e4efeaca9110613ee2c00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd012c2bb009453f893fc0c1f59b56f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a95327ead0e44961ab92b22ed888963a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0baacf17524d4a3f8483409f359aee5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fa99e4168bb4c798e5be10f34891fbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}